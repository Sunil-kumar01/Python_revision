# ğŸ“š TIME SERIES FORECASTING: Complete Technical Documentation

## Table of Contents
1. [Project Objectives](#project-objectives)
2. [Data Preprocessing](#data-preprocessing)
3. [Exploratory Data Analysis (EDA)](#exploratory-data-analysis)
4. [Stationarity & Testing](#stationarity-testing)
5. [Model Techniques](#model-techniques)
6. [Univariate vs Multivariate](#univariate-vs-multivariate)
7. [Challenges & Solutions](#challenges-solutions)
8. [Glossary of Terms](#glossary-of-terms)

---

## ğŸ¯ Project Objectives

### Business Objective
**Optimize commodity hedging decisions for AB InBev to minimize procurement costs**

**Specific Goals:**
- Forecast commodity prices 90 days ahead with >85% accuracy
- Provide T-policy recommendations (when to lock futures contracts)
- Quantify expected savings from optimal timing
- Handle streaming data for real-time decisions

### Technical Objectives
1. **Multi-granularity data pipeline**: Hourly â†’ Daily â†’ Monthly â†’ Yearly
2. **Comprehensive EDA**: Understand data structure before modeling
3. **Multiple model approaches**: Statistical (ARIMA, SARIMA) + ML (XGBoost)
4. **Production-ready system**: Automated retraining, monitoring, versioning

### Success Metrics
- **Forecast Accuracy**: MAPE < 5% for 30-day, < 10% for 90-day
- **Business Impact**: $2-5M annual savings per commodity
- **System Reliability**: 99.9% uptime, <100ms inference latency

---

## ğŸ“Š Data Preprocessing

### Multi-Granularity Approach

**Why multiple granularities?**
- Different stakeholders need different views (traders want hourly, executives want monthly)
- Some patterns only visible at certain frequencies
- Allows model comparison across time scales

#### 1. Hourly Data (Highest frequency)
```python
# Characteristics:
- 10 years Ã— ~252 trading days Ã— 8 hours = ~20,000 records
- Trading hours: 8 AM - 4 PM
- Captures intraday volatility
- Missing data: Weekends, holidays

# Use cases:
- High-frequency trading strategies
- Intraday volatility modeling
- Real-time price monitoring
```

**Challenges:**
- âŒ Large data volume (storage, computation)
- âŒ Many missing periods (non-trading hours)
- âŒ High noise-to-signal ratio
- âŒ Weekend/holiday handling

**Solutions:**
- âœ… Only store trading hours (reduce 67% of data)
- âœ… Forward-fill for short gaps (<3 hours)
- âœ… Aggregate to daily for modeling
- âœ… Use business day calendar

#### 2. Daily Data (Standard for forecasting)
```python
# Aggregation from hourly:
- Open: First hour (8 AM)
- High: Maximum during day
- Low: Minimum during day
- Close: Last hour (3 PM)
- Volume: Sum of all hours

# OHLC (Open-High-Low-Close) pattern:
- Standard in financial markets
- Captures full daily range
- Enables candlestick charts
```

**Why OHLC matters:**
- Shows daily price range (volatility)
- Open-Close: Daily trend direction
- High-Low: Intraday extremes
- Used in technical analysis

#### 3. Monthly Data (Business reporting)
```python
# Aggregation from daily:
- Month-end price: Last trading day
- Average price: Mean of all days
- Volatility: Standard deviation
- Total volume: Sum

# Use cases:
- Monthly P&L reporting
- Seasonal pattern analysis
- Budget planning
```

#### 4. Yearly Data (Strategic planning)
```python
# Aggregation from monthly:
- Year-end price
- Annual average
- Annual high/low
- Total annual volume

# Use cases:
- Long-term trend analysis
- Annual budgeting
- Strategic contracts
```

### Missing Value Handling

**Common causes in time series:**
1. Market closures (weekends, holidays)
2. System outages
3. Data collection failures
4. Sensor malfunctions

**Methods compared:**

| Method | Formula | When to Use | Pros | Cons |
|--------|---------|-------------|------|------|
| **Forward Fill** | `Y(t) = Y(t-1)` | Prices (persist) | Simple, realistic | Can't handle long gaps |
| **Backward Fill** | `Y(t) = Y(t+1)` | Future planning | Uses known future | Looks into future |
| **Linear Interpolation** | `Y(t) = Y(t-1) + (Y(t+1)-Y(t-1))/2` | Smooth trends | Smooth | Assumes linearity |
| **Mean/Median** | `Y(t) = mean(Y)` | Stable series | Simple | Ignores trend |
| **Seasonal Fill** | `Y(t) = Y(t-365)` | Strong seasonality | Uses pattern | Needs full cycle |

**Our approach: Forward Fill**
- Realistic for commodity prices (don't jump instantly)
- Max gap: 3 days (flag for review if longer)
- Production: Alert if gap > 1 day

---

## ğŸ” Exploratory Data Analysis (EDA)

### Why EDA is Critical

**"Look at your data first!" - Every experienced data scientist**

EDA helps you:
1. Understand data structure
2. Identify patterns
3. Detect anomalies
4. Guide model selection
5. Set realistic expectations

### 1. Time Series Decomposition

**Objective: Break series into interpretable components**

#### Formula (Additive Model):
```
Y(t) = T(t) + S(t) + R(t)

Where:
Y(t) = Observed value
T(t) = Trend component
S(t) = Seasonal component
R(t) = Residual (noise)
```

#### Formula (Multiplicative Model):
```
Y(t) = T(t) Ã— S(t) Ã— R(t)

Use when seasonal variation proportional to level
```

#### Components Explained:

**TREND (T):**
- Long-term direction (up/down/flat)
- Driven by: Inflation, demand growth, technology
- For commodities: Often upward (inflation) with cycles
- Interview: "Overall directional movement over time"

**Visualization:**
```
Price
â”‚     Trend â†’  â•±
â”‚            â•±
â”‚          â•±
â”‚        â•±
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time
```

**SEASONALITY (S):**
- Repeating patterns at fixed intervals
- Period: Daily, weekly, monthly, yearly
- For commodities: **HARVEST CYCLES** (most important!)
- Interview: "Predictable patterns due to calendar effects"

**Commodity seasonality example:**
```
Corn Price Pattern (Yearly):

High â”‚    â•±â•²        Planting season
     â”‚   â•±  â•²       (high demand)
     â”‚  â•±    â•²
Low  â”‚ â•±      â•²___  Harvest season
     â”‚         â•²    (high supply)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     Jan  Mar  May  Jul  Sep  Nov
```

**RESIDUALS (R):**
- What's left after removing trend + seasonality
- Should look like **white noise** (random)
- If patterns remain â†’ missing components!
- Interview: "Unexplained variation, should be random"

**How to interpret residuals:**
- âœ… Good: Random scatter around zero
- âŒ Bad: Patterns, autocorrelation, heteroscedasticity

### 2. Stationarity Testing

**What is Stationarity?**

A time series is **stationary** if its statistical properties don't change over time:

1. **Constant mean**: E[Y(t)] = Î¼ (same for all t)
2. **Constant variance**: Var[Y(t)] = ÏƒÂ² (same for all t)  
3. **Constant autocovariance**: Cov[Y(t), Y(t-k)] depends only on k, not t

**Visual examples:**

```
STATIONARY:
Price
â”‚  ~~~~Â·~Â·~~~Â·~~  Mean stays constant
â”‚ Â·           Â·~   Variance stays constant
â”‚Â·                
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time

NON-STATIONARY:
Price
â”‚              Â·   Mean increases
â”‚           Â· Â·    Variance increases
â”‚        Â·Â·
â”‚    Â·Â·
â”‚ Â·
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time
```

**Why stationarity matters:**

âŒ **Non-stationary problems:**
- Mean reverts to changing level (unpredictable)
- Variance changes â†’ confidence intervals meaningless
- Spurious correlations (two trending series look related)
- ARIMA models assume stationarity!

âœ… **Stationarity benefits:**
- Stable statistical properties
- Reliable predictions
- Valid hypothesis tests
- Better model performance

### ADF Test (Augmented Dickey-Fuller)

**What it tests:**

```
Null Hypothesis (H0): Series has a UNIT ROOT â†’ Non-stationary
Alternative (H1): Series is stationary
```

**Unit Root Explained:**

A unit root means shocks have **permanent effects**:

```python
# Unit root process (random walk):
Y(t) = Y(t-1) + Îµ(t)

# If Îµ is a shock, it persists forever!
# Today's shock affects all future values
```

**Mathematics:**
```
Test equation:
Î”Y(t) = Î± + Î²Â·t + Î³Â·Y(t-1) + Î´â‚Â·Î”Y(t-1) + ... + Îµ(t)

If Î³ = 0 â†’ Unit root â†’ Non-stationary
```

**Interpretation:**

| p-value | Decision | Meaning |
|---------|----------|---------|
| < 0.01 | Reject H0 (99% confidence) | **Strongly stationary** |
| < 0.05 | Reject H0 (95% confidence) | **Stationary** |
| < 0.10 | Reject H0 (90% confidence) | Possibly stationary |
| > 0.10 | Fail to reject H0 | **Non-stationary** |

**ADF Statistic interpretation:**
- More negative = More stationary
- Compare to critical values (-1%, -5%, -10%)
- If ADF < Critical Value â†’ Stationary

**Example output:**
```
ADF Statistic: -4.123
p-value: 0.001
Critical Values:
  1%: -3.43
  5%: -2.86
  10%: -2.57

Interpretation:
âœ… -4.123 < -3.43 â†’ Reject H0
âœ… p-value = 0.001 < 0.05 â†’ Stationary
```

### KPSS Test (Kwiatkowski-Phillips-Schmidt-Shin)

**Key difference from ADF:**

```
Null Hypothesis (H0): Series is STATIONARY
Alternative (H1): Series is non-stationary

NOTE: OPPOSITE of ADF!
```

**Why use both ADF and KPSS?**

| ADF Result | KPSS Result | Interpretation |
|------------|-------------|----------------|
| Stationary | Stationary | âœ… **Definitely stationary** |
| Non-stat | Non-stat | âŒ **Definitely non-stationary** |
| Stationary | Non-stat | ğŸ¤” **Trend-stationary (differencing may not help)** |
| Non-stat | Stationary | ğŸ¤” **Near unit root (borderline case)** |

**Interpretation table:**

| p-value | Decision | Meaning |
|---------|----------|---------|
| > 0.10 | Fail to reject H0 | **Stationary** |
| 0.05-0.10 | Borderline | Possibly stationary |
| < 0.05 | Reject H0 | **Non-stationary** |

**Example:**
```
KPSS Statistic: 0.123
p-value: 0.08
Critical Values:
  1%: 0.739
  5%: 0.463
  10%: 0.347

Interpretation:
âœ… 0.123 < 0.347 â†’ Stationary
âœ… p-value = 0.08 > 0.05 â†’ Fail to reject H0 â†’ Stationary
```

### PP Test (Phillips-Perron)

**Similar to ADF but:**
- More robust to heteroskedasticity
- More robust to autocorrelation
- Uses non-parametric methods

**When to use:**
- Volatility clustering (GARCH effects)
- Structural breaks
- ADF gives ambiguous results

### How to Achieve Stationarity

If tests show non-stationarity, try:

#### 1. Differencing
```python
# First difference:
Î”Y(t) = Y(t) - Y(t-1)

# Second difference (if needed):
Î”Â²Y(t) = Î”Y(t) - Î”Y(t-1)
```

**Effect:**
- Removes trend
- Makes mean constant
- This is the "I" (Integrated) in ARIMA!

#### 2. Log Transform
```python
Y'(t) = log(Y(t))
```

**Effect:**
- Stabilizes variance
- Converts multiplicative to additive
- Makes % changes constant

#### 3. Detrending
```python
# Remove linear trend:
Y'(t) = Y(t) - (a + bÂ·t)

Where a, b from linear regression
```

---

## ğŸ“ˆ Moving Averages Analysis

### Simple Moving Average (SMA)

**Formula:**
```
MA(t, n) = [Y(t) + Y(t-1) + ... + Y(t-n+1)] / n
```

**Example (3-day MA):**
```
Day:   1    2    3    4    5
Price: 4.0  4.2  4.1  4.3  4.5
MA3:   -    -   4.1  4.2  4.3
              â†‘
           (4.0+4.2+4.1)/3
```

**Window selection guide:**

| Window | Period | Use Case |
|--------|--------|----------|
| 7 | Week | Short-term trend |
| 30 | Month | Medium-term trend |
| 90 | Quarter | Seasonal pattern |
| 180 | Half-year | Long-term trend |
| 365 | Year | Annual cycle |

**Trading signals:**
- Price > MA â†’ **Uptrend** (bullish)
- Price < MA â†’ **Downtrend** (bearish)
- MA crossovers â†’ **Trend change**

**Golden Cross:**
```
Short MA crosses above Long MA â†’ Strong buy signal

Price
â”‚        Short MA â•±
â”‚              â•± â•± Long MA
â”‚           â•± â•±
â”‚        â•± â•±
â”‚     â•± â•±  â† Golden Cross
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time
```

### Exponential Moving Average (EMA)

**Formula:**
```
EMA(t) = Î±Â·Y(t) + (1-Î±)Â·EMA(t-1)

Where Î± = smoothing factor (0 to 1)
```

**Weight decay:**
```
Weight(t-k) = Î±Â·(1-Î±)^k

Example with Î± = 0.3:
Today:     0.30
Yesterday: 0.21 (= 0.3 Ã— 0.7)
2 days ago: 0.147 (= 0.3 Ã— 0.7Â²)
3 days ago: 0.103
...
```

**Advantage over SMA:**
- More responsive to recent changes
- Smooth transition (no sudden jumps)
- Used in MACD indicator

---

## ğŸ¯ Model Techniques

### 1. ARIMA Family

#### ARIMA(p,d,q)

**Components:**
- **AR(p)**: AutoRegressive
- **I(d)**: Integrated (Differencing)
- **MA(q)**: Moving Average

**Full equation:**
```
Ï†(B)Â·(1-B)^dÂ·Y(t) = Î¸(B)Â·Îµ(t)

Where:
Ï†(B) = AR polynomial
Î¸(B) = MA polynomial
B = Backshift operator
Îµ(t) = White noise
```

**Expanded form:**
```
Y(t) = c + Ï†â‚Â·Y(t-1) + Ï†â‚‚Â·Y(t-2) + ... + Ï†â‚šÂ·Y(t-p)
       + Î¸â‚Â·Îµ(t-1) + Î¸â‚‚Â·Îµ(t-2) + ... + Î¸_qÂ·Îµ(t-q)
       + Îµ(t)
```

**How to choose p, d, q:**

1. **d (Differencing):**
   - Run ADF test
   - If non-stationary: d=1
   - If still non-stationary after 1st diff: d=2
   - Rarely need d>2

2. **p (AR order) - Look at PACF:**
   ```
   PACF cuts off at lag p â†’ AR(p)
   
   Example:
   Lag:  1    2    3    4
   PACF: 0.8  0.4  0.05 0.02
         â†‘    â†‘    â†‘
         Sig  Sig  Not sig â†’ p=2
   ```

3. **q (MA order) - Look at ACF:**
   ```
   ACF cuts off at lag q â†’ MA(q)
   
   Example:
   Lag: 1    2    3    4
   ACF: 0.7  0.3  0.02 0.01
        â†‘    â†‘    â†‘
        Sig  Sig  Not sig â†’ q=2
   ```

#### SARIMA(p,d,q)(P,D,Q,s)

**Adds seasonal component:**

```
SARIMA equation:
Ï†(B)Â·Î¦(B^s)Â·(1-B)^dÂ·(1-B^s)^DÂ·Y(t) = Î¸(B)Â·Î˜(B^s)Â·Îµ(t)

Where:
Î¦(B^s) = Seasonal AR
Î˜(B^s) = Seasonal MA
s = Seasonal period
```

**Seasonal periods:**
- Hourly data, daily pattern: s=24
- Daily data, weekly pattern: s=7
- Daily data, monthly pattern: s=30
- Daily data, yearly pattern: s=365
- Monthly data, yearly pattern: s=12

**Example: SARIMA(1,1,1)(1,1,1,12)**
```
Non-seasonal: (1,1,1)
- 1st order AR
- 1st order differencing
- 1st order MA

Seasonal: (1,1,1,12)
- 1st order seasonal AR
- 1st order seasonal differencing
- 1st order seasonal MA
- Period of 12 months
```

#### SARIMAX

**Adds exogenous variables:**

```
SARIMAX equation:
Y(t) = SARIMA_part + Î²Â·X(t) + Îµ(t)

Where:
X(t) = External variables
Î² = Coefficients
```

**Example exogenous variables for commodities:**
- Weather: Temperature, rainfall
- Economic: GDP, inflation, unemployment
- Energy: Crude oil price, natural gas
- Demand: Ethanol production, livestock feed demand
- Supply: Planted acres, yield forecasts
- Currency: USD strength (for international commodities)

**Critical requirement:**
â— **Must have exogenous data for forecast period!**

---

### 2. Exponential Smoothing

#### Simple Exponential Smoothing

**Formula:**
```
Å·(t+1|t) = Î±Â·y(t) + (1-Î±)Â·Å·(t|t-1)
```

**Recursive form:**
```
Å·(t+1|t) = Å·(t|t-1) + Î±Â·[y(t) - Å·(t|t-1)]
                        â†‘
                   Forecast error
```

**Choosing Î±:**
- Î± near 0: Slow adaptation (smooth)
- Î± near 1: Fast adaptation (responsive)
- Optimize via MSE minimization

#### Holt's Method (Double Exponential)

**Adds trend:**

```
Level:  â„“(t) = Î±Â·y(t) + (1-Î±)Â·[â„“(t-1) + b(t-1)]
Trend:  b(t) = Î²Â·[â„“(t) - â„“(t-1)] + (1-Î²)Â·b(t-1)

Forecast: Å·(t+h|t) = â„“(t) + hÂ·b(t)
```

#### Holt-Winters (Triple Exponential)

**Adds seasonality:**

**Additive model:**
```
Level:    â„“(t) = Î±Â·[y(t) - s(t-m)] + (1-Î±)Â·[â„“(t-1) + b(t-1)]
Trend:    b(t) = Î²Â·[â„“(t) - â„“(t-1)] + (1-Î²)Â·b(t-1)
Seasonal: s(t) = Î³Â·[y(t) - â„“(t)] + (1-Î³)Â·s(t-m)

Forecast: Å·(t+h|t) = â„“(t) + hÂ·b(t) + s(t+h-m)
```

**Multiplicative model:**
```
Forecast: Å·(t+h|t) = [â„“(t) + hÂ·b(t)]Â·s(t+h-m)
```

---

### 3. Vector Models (Multivariate)

#### VARMA(p, q)

**Vector AutoRegressive Moving Average for k variables:**

```
Y(t) = c + Aâ‚Â·Y(t-1) + Aâ‚‚Â·Y(t-2) + ... + Aâ‚šÂ·Y(t-p)
       + Mâ‚Â·Îµ(t-1) + Mâ‚‚Â·Îµ(t-2) + ... + M_qÂ·Îµ(t-q)
       + Îµ(t)

Where:
Y(t) = [Yâ‚(t), Yâ‚‚(t), ..., Y_k(t)]' (kÃ—1 vector)
A_i = kÃ—k matrices
M_j = kÃ—k matrices
Îµ(t) = kÃ—1 error vector
```

**Example with k=2 (Corn, Wheat):**

```
Corn(t) = câ‚ + aâ‚â‚Â·Corn(t-1) + aâ‚â‚‚Â·Wheat(t-1) + Îµâ‚(t)
Wheat(t) = câ‚‚ + aâ‚‚â‚Â·Corn(t-1) + aâ‚‚â‚‚Â·Wheat(t-1) + Îµâ‚‚(t)

Cross-effects:
aâ‚â‚‚: How wheat affects corn
aâ‚‚â‚: How corn affects wheat
```

**Parameter count:**
```
Total parameters = kÂ² Ã— p + kÂ² Ã— q + k

For k=3, p=2, q=1:
= 3Â² Ã— 2 + 3Â² Ã— 1 + 3
= 18 + 9 + 3
= 30 parameters!
```

#### VARMAX

**Adds exogenous variables:**

```
Y(t) = c + Aâ‚Â·Y(t-1) + ... + Aâ‚šÂ·Y(t-p)
       + Mâ‚Â·Îµ(t-1) + ... + M_qÂ·Îµ(t-q)
       + BÂ·X(t)  â† Exogenous
       + Îµ(t)
```

---

## ğŸ”„ Univariate vs Multivariate

### Univariate Analysis

**Definition:** Model one variable using its own history

**Advantages:**
âœ… Simpler to understand
âœ… Fewer parameters
âœ… Less data needed
âœ… Faster to train
âœ… Easier to interpret

**Disadvantages:**
âŒ Ignores relationships with other variables
âŒ May miss important drivers
âŒ Separate model for each variable

**When to use:**
- One primary variable of interest
- Variables truly independent
- Limited data
- Need interpretability

**Models:**
- ARIMA, SARIMA, SARIMAX
- Exponential Smoothing
- Single-variable XGBoost

### Multivariate Analysis

**Definition:** Model multiple variables jointly

**Advantages:**
âœ… Captures cross-variable dynamics
âœ… One model for all variables
âœ… Better when variables related
âœ… Granger causality testing

**Disadvantages:**
âŒ Complex
âŒ Many parameters
âŒ Needs more data
âŒ Curse of dimensionality
âŒ Harder to interpret

**When to use:**
- Variables influence each other
- Need joint forecasts
- Sufficient data
- System dynamics important

**Models:**
- VARMA, VARMAX
- Dynamic Factor Models
- Multivariate XGBoost

---

## âš ï¸ Challenges & Solutions

### Challenge 1: Non-Stationarity

**Problem:**
```
Prices trend upward â†’ Mean not constant â†’ ARIMA fails
```

**Solutions:**
1. **Differencing:**
   ```python
   Y'(t) = Y(t) - Y(t-1)  # 1st difference
   ```

2. **Log transform:**
   ```python
   Y'(t) = log(Y(t))
   ```

3. **Detrending:**
   ```python
   Y'(t) = Y(t) - trend(t)
   ```

### Challenge 2: Seasonality

**Problem:**
```
Harvest cycles create strong patterns â†’ ARIMA not enough
```

**Solutions:**
1. **Use SARIMA:**
   ```python
   SARIMA(1,1,1)(1,1,1,365)  # Yearly seasonality
   ```

2. **Seasonal differencing:**
   ```python
   Y'(t) = Y(t) - Y(t-365)
   ```

3. **Dummy variables:**
   ```python
   month_1, month_2, ..., month_12
   ```

### Challenge 3: Multiple Time Granularities

**Problem:**
```
Need hourly (trading), daily (analysis), monthly (reports)
```

**Solutions:**
1. **Hierarchical forecasting:**
   ```
   - Forecast daily
   - Disaggregate to hourly
   - Aggregate to monthly
   ```

2. **Store all granularities:**
   ```
   - Raw: Hourly
   - Modeling: Daily
   - Reporting: Monthly
   ```

### Challenge 4: Missing Data

**Problem:**
```
Holidays, weekends, system outages â†’ Gaps in data
```

**Solutions:**
1. **Forward fill (our choice):**
   ```python
   df.fillna(method='ffill')
   ```

2. **Seasonal fill:**
   ```python
   df[t] = df[t-365]  # Last year same day
   ```

3. **Alert on long gaps:**
   ```python
   if gap > 3 days: send_alert()
   ```

### Challenge 5: Computational Cost

**Problem:**
```
10 years Ã— 5 commodities Ã— hourly = 350K rows
VARMAX with 5 variables Ã— 100 parameters = Slow!
```

**Solutions:**
1. **Downsampling:**
   ```
   Train on daily instead of hourly
   ```

2. **Parallelization:**
   ```python
   joblib.Parallel(n_jobs=-1)
   ```

3. **Model selection:**
   ```
   Use simpler models (ARIMA) first
   Complex models (VARMAX) only if needed
   ```

---

## ğŸ“– Glossary of Terms

### A
**ACF (AutoCorrelation Function):**
Correlation between series and its lagged values.

**ADF (Augmented Dickey-Fuller) Test:**
Statistical test for stationarity.

**AR (AutoRegressive):**
Model where current value depends on past values.

**ARIMA:**
AutoRegressive Integrated Moving Average model.

### D
**Differencing:**
Subtracting previous value to achieve stationarity.

### E
**Endogenous Variable:**
Variable predicted by the model using its own history.

**Exogenous Variable:**
External variable that influences target but isn't predicted.

**Exponential Smoothing:**
Forecasting method that weights recent observations more.

### G
**Granger Causality:**
Statistical test if one time series helps predict another.

### H
**Heteroskedasticity:**
Non-constant variance over time.

**Holt-Winters:**
Triple exponential smoothing with trend and seasonality.

### K
**KPSS Test:**
Stationarity test (null hypothesis: stationary).

### L
**Lag:**
Previous time period (lag 1 = yesterday).

### M
**MA (Moving Average in ARIMA context):**
Model based on past forecast errors.

**MAPE (Mean Absolute Percentage Error):**
Average absolute % error.

### P
**PACF (Partial AutoCorrelation Function):**
Correlation after removing intermediate effects.

### R
**Residuals:**
Difference between actual and predicted values.

### S
**SARIMA:**
Seasonal ARIMA.

**SARIMAX:**
SARIMA with exogenous variables.

**Seasonality:**
Repeating pattern at fixed intervals.

**Stationarity:**
Constant mean, variance, and covariance over time.

### T
**Trend:**
Long-term direction of series.

### U
**Unit Root:**
Series has permanent response to shocks (non-stationary).

### V
**VARMA:**
Vector AutoRegressive Moving Average (multivariate).

**VARMAX:**
VARMA with exogenous variables.

### W
**White Noise:**
Random series with zero mean, constant variance, no autocorrelation.

---

## ğŸ“ Interview Quick Reference

**Key points to mention:**

1. **Stationarity is critical** - Always test with ADF/KPSS
2. **Decomposition first** - Understand trend, seasonality, residuals
3. **Multiple models** - Compare ARIMA, SARIMA, Exp Smoothing
4. **Production considerations** - Retraining, monitoring, versioning
5. **Business value** - $2-5M savings, not just accuracy metrics
6. **Challenges handled** - Missing data, non-stationarity, seasonality

**Be ready to explain:**
- What is stationarity and why it matters
- Difference between AR and MA
- How to read ACF/PACF plots
- When to use multivariate vs univariate
- How to choose ARIMA parameters
- T-policy business logic

---

*This documentation is comprehensive and interview-ready!* ğŸš€
