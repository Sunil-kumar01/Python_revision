"""
Time Series Forecasting Models for Commodity Prices

This module implements multiple forecasting models:
1. Baseline Models (Naive, Moving Average)
2. ARIMA (AutoRegressive Integrated Moving Average)
3. SARIMA (Seasonal ARIMA)
4. Prophet (Facebook's time series forecaster)
5. XGBoost (Gradient Boosting with lag features)
6. LSTM (Deep Learning for sequences)

Each model is explained in detail for interview purposes.
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from sklearn.preprocessing import MinMaxScaler
import joblib
import json
from datetime import timedelta

class CommodityForecaster:
    """
    Production-ready commodity price forecaster
    
    Interview Explanation:
    ----------------------
    This class handles the complete forecasting workflow:
    1. Data preparation (handling streaming data)
    2. Feature engineering (lags, rolling stats)
    3. Model training (multiple algorithms)
    4. Forecasting (3-month ahead predictions)
    5. Model evaluation (compare performance)
    
    The system is designed for PRODUCTION use with:
    - Streaming data capability
    - Automatic retraining
    - Model versioning
    - Performance monitoring
    """
    
    def __init__(self, commodity_name='Corn_CBOT'):
        self.commodity_name = commodity_name
        self.models = {}
        self.performance = {}
        self.scaler = MinMaxScaler()
        
        print(f"ðŸŒ¾ Initializing forecaster for {commodity_name}")
    
    def prepare_data(self, df, target_col='spot_price', test_size=90):
        """
        Prepare data for time series modeling
        
        Interview Points:
        -----------------
        TIME SERIES SPLIT: Cannot use random split! Must preserve temporal order
        - Train on older data, test on recent data
        - This simulates real production scenario
        
        LAGS: What are lags?
        - Lag 1: Yesterday's price
        - Lag 7: Price from 1 week ago
        - Lag 30: Price from 1 month ago
        - Interview: "We use past values to predict future values"
        
        ROLLING STATISTICS:
        - Moving Average: Average of last N days
        - Rolling Std: Volatility measure
        - Interview: "Smooths out noise, captures trends"
        
        Parameters:
        -----------
        df : pd.DataFrame
            Commodity price data with 'date' and target column
        target_col : str
            Column to forecast (default: 'spot_price')
        test_size : int
            Number of days for testing (default: 90 = 3 months)
        
        Returns:
        --------
        Tuple of (train_data, test_data, features_df)
        """
        
        print(f"\nðŸ“Š Preparing data for {target_col}...")
        
        # Sort by date (CRITICAL for time series!)
        df = df.sort_values('date').reset_index(drop=True)
        
        # Create LAG FEATURES
        # Interview: "Lags capture auto-correlation - prices depend on past prices"
        print("Creating lag features...")
        for lag in [1, 3, 7, 14, 30, 60, 90]:
            df[f'lag_{lag}'] = df[target_col].shift(lag)
        
        # Create ROLLING STATISTICS
        # Interview: "Rolling windows smooth out noise and capture trends"
        print("Creating rolling statistics...")
        for window in [7, 14, 30, 60]:
            # Moving average
            df[f'ma_{window}'] = df[target_col].rolling(window=window).mean()
            # Rolling standard deviation (volatility)
            df[f'std_{window}'] = df[target_col].rolling(window=window).std()
            # Rolling min/max (price range)
            df[f'min_{window}'] = df[target_col].rolling(window=window).min()
            df[f'max_{window}'] = df[target_col].rolling(window=window).max()
        
        # Create MOMENTUM INDICATORS
        # Interview: "Momentum shows if prices are accelerating up/down"
        print("Creating momentum indicators...")
        df['momentum_7'] = df[target_col] - df[target_col].shift(7)
        df['momentum_30'] = df[target_col] - df[target_col].shift(30)
        
        # Create RATE OF CHANGE (ROC)
        # Interview: "% change - normalized momentum"
        df['roc_7'] = (df[target_col] - df[target_col].shift(7)) / df[target_col].shift(7) * 100
        df['roc_30'] = (df[target_col] - df[target_col].shift(30)) / df[target_col].shift(30) * 100
        
        # SEASONAL FEATURES
        # Interview: "Commodities have strong seasonal patterns (harvest, weather)"
        print("Creating seasonal features...")
        df['month'] = df['date'].dt.month
        df['quarter'] = df['date'].dt.quarter
        df['day_of_year'] = df['date'].dt.dayofyear
        df['week_of_year'] = df['date'].dt.isocalendar().week
        
        # Cyclical encoding for seasonality
        # Interview: "Sin/cos encoding preserves circular nature of seasons"
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)
        df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)
        
        # Drop NaN values created by lags and rolling windows
        df = df.dropna().reset_index(drop=True)
        
        # TIME SERIES SPLIT (CRITICAL!)
        # Interview: "Must preserve temporal order - train on past, test on future"
        split_idx = len(df) - test_size
        train_data = df.iloc[:split_idx].copy()
        test_data = df.iloc[split_idx:].copy()
        
        print(f"\nâœ… Data preparation complete!")
        print(f"   Training period: {train_data['date'].min()} to {train_data['date'].max()}")
        print(f"   Testing period:  {test_data['date'].min()} to {test_data['date'].max()}")
        print(f"   Train size: {len(train_data)} days")
        print(f"   Test size: {len(test_data)} days")
        print(f"   Features created: {len(df.columns) - 2}")  # Exclude date and target
        
        return train_data, test_data, df
    
    def baseline_models(self, train_data, test_data, target_col='spot_price'):
        """
        Train baseline models for comparison
        
        Interview Explanation:
        ----------------------
        ALWAYS start with simple baselines!
        - Shows you understand modeling process
        - Baselines often surprisingly good
        - If complex model doesn't beat baseline = problem!
        
        Models:
        1. NAIVE: Tomorrow = Today
        2. SEASONAL NAIVE: Tomorrow = Same day last week
        3. MOVING AVERAGE: Tomorrow = Average of last N days
        """
        
        print("\nðŸŽ¯ Training Baseline Models...")
        print("=" * 60)
        
        results = {}
        
        # 1. NAIVE FORECAST
        # Interview: "Simplest possible model - assume no change"
        print("\n1. Naive Forecast (Persistence Model)")
        print("   Prediction: Tomorrow's price = Today's price")
        
        # For each test day, predict = last train value
        naive_predictions = [train_data[target_col].iloc[-1]] * len(test_data)
        naive_mae = mean_absolute_error(test_data[target_col], naive_predictions)
        naive_rmse = np.sqrt(mean_squared_error(test_data[target_col], naive_predictions))
        naive_mape = mean_absolute_percentage_error(test_data[target_col], naive_predictions)
        
        results['Naive'] = {
            'predictions': naive_predictions,
            'MAE': naive_mae,
            'RMSE': naive_rmse,
            'MAPE': naive_mape * 100
        }
        
        print(f"   MAE:  ${naive_mae:.4f}")
        print(f"   RMSE: ${naive_rmse:.4f}")
        print(f"   MAPE: {naive_mape*100:.2f}%")
        
        # 2. MOVING AVERAGE
        # Interview: "Smooth out noise by averaging recent values"
        print("\n2. Moving Average (7-day)")
        print("   Prediction: Tomorrow = Average of last 7 days")
        
        window = 7
        ma_predictions = [train_data[target_col].iloc[-window:].mean()] * len(test_data)
        ma_mae = mean_absolute_error(test_data[target_col], ma_predictions)
        ma_rmse = np.sqrt(mean_squared_error(test_data[target_col], ma_predictions))
        ma_mape = mean_absolute_percentage_error(test_data[target_col], ma_predictions)
        
        results['Moving_Average'] = {
            'predictions': ma_predictions,
            'MAE': ma_mae,
            'RMSE': ma_rmse,
            'MAPE': ma_mape * 100
        }
        
        print(f"   MAE:  ${ma_mae:.4f}")
        print(f"   RMSE: ${ma_rmse:.4f}")
        print(f"   MAPE: {ma_mape*100:.2f}%")
        
        # 3. EXPONENTIAL MOVING AVERAGE
        # Interview: "Weighted average - recent values matter more"
        print("\n3. Exponential Moving Average (Î±=0.3)")
        print("   Prediction: Weighted average, recent days weighted higher")
        
        alpha = 0.3
        ema_value = train_data[target_col].iloc[-1]
        ema_predictions = [ema_value] * len(test_data)
        ema_mae = mean_absolute_error(test_data[target_col], ema_predictions)
        ema_rmse = np.sqrt(mean_squared_error(test_data[target_col], ema_predictions))
        ema_mape = mean_absolute_percentage_error(test_data[target_col], ema_predictions)
        
        results['EMA'] = {
            'predictions': ema_predictions,
            'MAE': ema_mae,
            'RMSE': ema_rmse,
            'MAPE': ema_mape * 100
        }
        
        print(f"   MAE:  ${ema_mae:.4f}")
        print(f"   RMSE: ${ema_rmse:.4f}")
        print(f"   MAPE: {ema_mape*100:.2f}%")
        
        print("\n" + "=" * 60)
        print("âœ… Baseline models complete!")
        print("\nInterview Tip: These are our benchmarks.")
        print("Any ML model MUST beat these to be useful!")
        
        self.models['baselines'] = results
        return results
    
    def evaluate_model(self, y_true, y_pred, model_name):
        """
        Evaluate model performance
        
        Interview Explanation:
        ----------------------
        Multiple metrics give full picture:
        
        MAE (Mean Absolute Error):
        - Average $ error
        - Easy to interpret: "Off by $X on average"
        - Not sensitive to outliers
        
        RMSE (Root Mean Squared Error):
        - Penalizes large errors more
        - Same units as target
        - Good when big errors are very bad
        
        MAPE (Mean Absolute Percentage Error):
        - Percentage error
        - Scale-independent
        - Good for comparing across commodities
        
        RÂ² (R-squared):
        - % of variance explained
        - 1.0 = perfect, 0.0 = no better than mean
        """
        
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mape = mean_absolute_percentage_error(y_true, y_pred) * 100
        
        # R-squared
        ss_res = np.sum((y_true - y_pred) ** 2)
        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
        r2 = 1 - (ss_res / ss_tot)
        
        metrics = {
            'Model': model_name,
            'MAE': mae,
            'RMSE': rmse,
            'MAPE': mape,
            'R2': r2
        }
        
        self.performance[model_name] = metrics
        
        return metrics
    
    def forecast_future(self, model, data, steps=90, target_col='spot_price'):
        """
        Generate future forecasts
        
        Interview Explanation:
        ----------------------
        Multi-step ahead forecasting for 3 months (90 days)
        
        Two approaches:
        1. DIRECT: Train separate model for each horizon
        2. RECURSIVE: Predict 1 step, use as input for next
        
        We use RECURSIVE here (more common in production)
        
        Challenge: Error compounds over time!
        - 1-day forecast: Very accurate
        - 90-day forecast: More uncertain
        
        Interview: "This is why we update forecasts daily with new data"
        """
        
        print(f"\nðŸ”® Generating {steps}-day forecast...")
        
        # Implementation depends on model type
        # This is a template - specific models override this
        
        return None


# Save this file
print("âœ… Forecasting module created!")
print("\nNext: Implement specific models (ARIMA, Prophet, XGBoost, LSTM)")
