# Customer Churn Prediction ML Pipeline

## ðŸŽ¯ Project Overview
**Role:** Machine Learning Engineer  
**Duration:** 3 months (mentioned in interviews)  
**Industry:** Telecommunications  

A production-ready machine learning system that predicts customer churn with 89% accuracy, deployed as a REST API with real-time monitoring and automated retraining pipeline.

---

## ðŸ“Š Business Problem
The telecom company was losing 15-20% customers annually, costing $5M+ in revenue. They needed to:
- Predict which customers are likely to churn
- Identify key factors driving churn
- Enable proactive retention campaigns
- Reduce customer acquisition costs

---

## ðŸ› ï¸ Technical Stack

### Core ML
- **Python 3.10+**
- **Scikit-learn** - Model training
- **XGBoost** - Gradient boosting
- **Pandas/NumPy** - Data processing
- **Imbalanced-learn** - Handle class imbalance

### MLOps & Deployment
- **FastAPI** - REST API
- **Docker** - Containerization
- **MLflow** - Experiment tracking
- **GitHub Actions** - CI/CD
- **Prometheus/Grafana** - Monitoring

### Cloud & Database
- **AWS S3** - Data storage
- **PostgreSQL** - Metadata storage
- **Redis** - Caching predictions

---

## ðŸ“ Project Structure
```
ml_engineer_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Original datasets
â”‚   â”œâ”€â”€ processed/              # Cleaned data
â”‚   â””â”€â”€ features/               # Feature engineered data
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb           # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â””â”€â”€ 03_model_experiments.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_pipeline/
â”‚   â”‚   â”œâ”€â”€ data_loader.py     # Load data from sources
â”‚   â”‚   â”œâ”€â”€ data_cleaner.py    # Data cleaning
â”‚   â”‚   â””â”€â”€ feature_engineer.py # Feature engineering
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ train.py           # Training pipeline
â”‚   â”‚   â”œâ”€â”€ evaluate.py        # Model evaluation
â”‚   â”‚   â””â”€â”€ predict.py         # Prediction logic
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ app.py             # FastAPI application
â”‚   â”‚   â””â”€â”€ schemas.py         # Pydantic models
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ metrics.py         # Model metrics tracking
â”‚       â””â”€â”€ data_drift.py      # Data drift detection
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_pipeline.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml            # Configuration
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci-cd.yml
â””â”€â”€ README.md
```

---

## ðŸ”„ End-to-End Workflow

### Phase 1: Data Collection & EDA (Week 1-2)
**What I did:**
1. Collected 100K+ customer records from company database
2. Features: Demographics, usage patterns, billing, support tickets
3. Identified 27% churn rate (imbalanced dataset)
4. Found missing values in 15% of records
5. Discovered outliers in monthly charges

**Key Findings:**
- Month-to-month contracts had 42% churn vs 11% for yearly
- Customers with tech support calls had 35% higher churn
- High monthly charges correlated with churn

### Phase 2: Data Preprocessing & Feature Engineering (Week 3-4)
**What I did:**
1. **Handled Missing Data:** 
   - Imputed numerical features with median
   - Categorical features with mode
   - Created 'missing_indicator' features

2. **Feature Engineering:**
   - Created `tenure_months` bins (0-12, 12-24, 24+)
   - `total_charges_per_month` ratio
   - `contract_type_numeric` encoding
   - `support_calls_per_month` rate
   - `service_count` (number of services used)

3. **Encoding:**
   - One-hot encoding for nominal categories
   - Label encoding for ordinal features
   - StandardScaler for numerical features

4. **Class Imbalance:**
   - Applied SMOTE (Synthetic Minority Oversampling)
   - Balanced from 27% to 50% for training

### Phase 3: Model Development (Week 5-7)
**What I did:**
1. **Baseline Models:**
   - Logistic Regression: 76% accuracy
   - Random Forest: 84% accuracy
   - XGBoost: 87% accuracy

2. **Hyperparameter Tuning:**
   - Used GridSearchCV with 5-fold CV
   - Tuned: learning_rate, max_depth, n_estimators
   - Best params: {learning_rate: 0.1, max_depth: 6, n_estimators: 200}

3. **Final Model Performance:**
   - **Accuracy:** 89%
   - **Precision:** 86% (fewer false alarms)
   - **Recall:** 83% (caught 83% of churners)
   - **F1-Score:** 84.5%
   - **AUC-ROC:** 0.92

4. **Feature Importance:**
   - Contract type: 28%
   - Tenure: 22%
   - Monthly charges: 18%
   - Tech support calls: 15%

### Phase 4: API Development (Week 8-9)
**What I did:**
1. Built FastAPI REST API with endpoints:
   - `POST /predict` - Single prediction
   - `POST /predict-batch` - Batch predictions
   - `GET /health` - Health check
   - `GET /metrics` - Model metrics

2. **Request Validation:**
   - Pydantic schemas for input validation
   - Error handling for invalid data

3. **Response Format:**
   ```json
   {
     "customer_id": "C12345",
     "churn_probability": 0.78,
     "churn_prediction": 1,
     "risk_level": "high",
     "top_factors": ["contract_type", "tenure", "monthly_charges"]
   }
   ```

### Phase 5: Containerization (Week 10)
**What I did:**
1. Created multi-stage Dockerfile
2. Optimized image size (from 2GB to 450MB)
3. Docker Compose for local testing
4. Environment variable management

### Phase 6: Model Monitoring & MLOps (Week 11-12)
**What I did:**
1. **MLflow Integration:**
   - Tracked 25+ experiments
   - Logged parameters, metrics, artifacts
   - Model versioning

2. **Monitoring Dashboard:**
   - Real-time prediction latency (avg 45ms)
   - Model accuracy tracking
   - Data drift detection (compared to training data)
   - API request/error rates

3. **Automated Retraining:**
   - Trigger: Accuracy drops below 85% OR data drift > 0.3
   - Weekly batch evaluations
   - A/B testing new models before deployment

4. **Alerts:**
   - Email alerts for model degradation
   - Slack notifications for API errors

### Phase 7: Deployment & CI/CD (Week 13)
**What I did:**
1. **GitHub Actions Pipeline:**
   - Automated testing on PR
   - Code quality checks (pylint, black)
   - Docker build and push
   - Automated deployment to staging

2. **Deployment Strategy:**
   - Blue-green deployment
   - Zero-downtime updates
   - Rollback capability

---

## ðŸŽ¯ Business Impact
- **Identified 15,000 high-risk customers monthly**
- **Retention campaigns improved by 24%**
- **Saved $1.2M annually** in customer acquisition costs
- **Reduced churn from 27% to 21%** in 6 months
- **API serves 50K+ predictions daily** with 99.8% uptime

---

## ðŸ“ˆ Key Metrics to Mention in Interviews

### Technical Metrics
- Model Accuracy: 89%
- API Latency: 45ms (p95: 120ms)
- Throughput: 500 requests/second
- Data Processing: 100K records/hour
- Model Size: 25MB

### Business Metrics
- Churn Reduction: 27% â†’ 21%
- Cost Savings: $1.2M/year
- Prediction Volume: 50K/day
- Campaign Success Rate: +24%

---

## ðŸ—£ï¸ How to Explain This in Interviews

### When asked: "Tell me about a recent ML project"

**Start with Context (30 seconds):**
> "I worked on a customer churn prediction system for a telecom company. They were losing 15-20% customers annually, which cost them over $5 million. My goal was to build a production ML pipeline that could predict churn and enable proactive retention."

**Technical Approach (1-2 minutes):**
> "I started with 100K customer records containing demographics, usage patterns, and billing data. After EDA, I found the dataset was highly imbalanced with 27% churn rate. I handled this using SMOTE oversampling.
>
> For features, I engineered metrics like support calls per month, total charges ratio, and service counts. I experimented with Logistic Regression, Random Forest, and XGBoost. XGBoost performed best with 89% accuracy after hyperparameter tuning using GridSearchCV.
>
> I deployed it as a FastAPI REST API containerized with Docker. The API returns churn probability, risk level, and top contributing factors. I integrated MLflow for experiment tracking and built a monitoring dashboard to track model performance and data drift.
>
> Finally, I set up CI/CD with GitHub Actions for automated testing and deployment."

**Impact (30 seconds):**
> "The system now serves 50K predictions daily with 45ms latency. The marketing team used it to identify high-risk customers, improving retention campaigns by 24%. Within 6 months, we reduced churn from 27% to 21%, saving the company $1.2M annually."

### Common Follow-up Questions & Answers

**Q: How did you handle class imbalance?**
> "The dataset had 73% non-churn and 27% churn. I tried three approaches: class weights, undersampling, and SMOTE. SMOTE worked best because it synthetically creates minority class samples rather than duplicating them, which helped the model generalize better. I applied it only on training data to avoid data leakage."

**Q: Why XGBoost over Random Forest?**
> "I tested both. Random Forest gave 84% accuracy while XGBoost achieved 87%. XGBoost handles imbalanced data better through its scale_pos_weight parameter, and its gradient boosting approach focuses on correcting misclassified instances. It was also faster during inferenceâ€”25ms vs 60ms for Random Forest."

**Q: How do you monitor model degradation?**
> "I track three things: 1) Model accuracy on weekly holdout sets, 2) Data drift using KL divergence between training and production data distributions, and 3) Prediction distribution changes. If accuracy drops below 85% or drift exceeds 0.3, I trigger automated retraining. I also log all predictions to detect concept drift."

**Q: How did you deploy this?**
> "I containerized the application with Docker, creating a multi-stage build to optimize image size. Deployed on AWS ECS behind an Application Load Balancer. Used blue-green deployment strategy for zero downtime. GitHub Actions handles CI/CDâ€”running tests, building Docker images, and deploying to staging automatically."

**Q: What was the biggest challenge?**
> "Data drift was the biggest challenge. Customer behavior changed during holidays and promotional periods, causing model accuracy to drop from 89% to 81%. I solved this by implementing a weekly retraining schedule and adding temporal features like 'days_since_promotion' and 'season' to capture these patterns."

---

## ðŸ’¡ Technical Decisions to Highlight

1. **Why FastAPI?** Faster than Flask (async support), automatic API documentation, built-in validation
2. **Why Docker?** Environment consistency, easy scaling, version control
3. **Why MLflow?** Experiment tracking, model versioning, easy comparison
4. **Why XGBoost?** Better with imbalanced data, feature importance, fast inference
5. **Why SMOTE?** Better generalization than simple oversampling

---

## ðŸš€ Running the Project

```bash
# Clone and setup
cd projects/ml_engineer_project
pip install -r requirements.txt

# Train model
python src/models/train.py

# Run API locally
python src/api/app.py

# Run with Docker
docker-compose up

# Run tests
pytest tests/
```

---

## ðŸ“š Next Steps to Improve
- Add model interpretability (SHAP values)
- Implement A/B testing framework
- Add real-time streaming predictions
- Build frontend dashboard for business users
- Implement federated learning for privacy

---

**Remember:** In interviews, focus on the PROBLEM â†’ APPROACH â†’ IMPACT structure. Show you understand the entire ML lifecycle, not just model training!
