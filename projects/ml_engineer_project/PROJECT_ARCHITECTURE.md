# ğŸ—ï¸ ML Engineer Project - Technical Architecture

## ğŸ“‹ Table of Contents

1. [System Overview](#system-overview)
2. [Architecture Diagram](#architecture-diagram)
3. [Component Details](#component-details)
4. [Data Flow](#data-flow)
5. [Technology Stack](#technology-stack)
6. [Deployment Architecture](#deployment-architecture)
7. [Scalability & Performance](#scalability--performance)
8. [Security](#security)
9. [Monitoring & Observability](#monitoring--observability)
10. [CI/CD Pipeline](#cicd-pipeline)

---

## ğŸ¯ System Overview

### Purpose
Production-ready customer churn prediction system that provides real-time predictions via REST API with automated monitoring and retraining capabilities.

### Key Characteristics
- **Type**: Machine Learning as a Service (MLaaS)
- **Architecture**: Microservices with containerization
- **Deployment**: Docker Compose (local/dev), Kubernetes (production)
- **Latency**: < 50ms p95
- **Throughput**: 50,000+ predictions/day
- **Availability**: 99.9% SLA
- **Scalability**: Horizontal scaling ready

---

## ğŸ“ Architecture Diagram

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CLIENT LAYER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Web App  â”‚  Mobile App  â”‚  Data Pipeline  â”‚  Admin Dashboard  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚             â”‚                 â”‚                  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API GATEWAY / LOAD BALANCER                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                 â”‚                 â”‚
          â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI App 1  â”‚ â”‚   FastAPI App 2  â”‚ â”‚   FastAPI App 3  â”‚
â”‚  (Port 8001)     â”‚ â”‚  (Port 8002)     â”‚ â”‚  (Port 8003)     â”‚
â”‚                  â”‚ â”‚                  â”‚ â”‚                  â”‚
â”‚  â€¢ /predict      â”‚ â”‚  â€¢ /predict      â”‚ â”‚  â€¢ /predict      â”‚
â”‚  â€¢ /health       â”‚ â”‚  â€¢ /health       â”‚ â”‚  â€¢ /health       â”‚
â”‚  â€¢ /batch        â”‚ â”‚  â€¢ /batch        â”‚ â”‚  â€¢ /batch        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚                    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚               â”‚               â”‚
              â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PostgreSQL DB  â”‚ â”‚  Redis   â”‚ â”‚  Model Store   â”‚
    â”‚                 â”‚ â”‚  Cache   â”‚ â”‚  (S3/Volume)   â”‚
    â”‚  â€¢ Predictions  â”‚ â”‚          â”‚ â”‚                â”‚
    â”‚  â€¢ Ground Truth â”‚ â”‚  â€¢ 5min  â”‚ â”‚  â€¢ model.pkl   â”‚
    â”‚  â€¢ Metrics      â”‚ â”‚    TTL   â”‚ â”‚  â€¢ metadata    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Monitoring     â”‚
    â”‚                 â”‚
    â”‚  â€¢ Prometheus   â”‚â”€â”€â”
    â”‚  â€¢ Grafana      â”‚  â”‚
    â”‚  â€¢ Alerts       â”‚  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                         â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RETRAINING PIPELINE             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Data Extraction (Last 6 months)     â”‚
â”‚  2. Data Validation & Cleaning          â”‚
â”‚  3. Feature Engineering                 â”‚
â”‚  4. Model Training (XGBoost)            â”‚
â”‚  5. Model Evaluation & A/B Test         â”‚
â”‚  6. Model Deployment (if improved)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA SOURCES                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CSV Files    â”‚  PostgreSQL   â”‚  MySQL        â”‚  AWS S3       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚               â”‚               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         DATA LOADER (data_loader.py)       â”‚
        â”‚                                            â”‚
        â”‚  â€¢ Connection management                   â”‚
        â”‚  â€¢ Data validation                         â”‚
        â”‚  â€¢ Error handling                          â”‚
        â”‚  â€¢ Logging                                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       DATA CLEANER (data_cleaner.py)       â”‚
        â”‚                                            â”‚
        â”‚  1. Handle missing values                  â”‚
        â”‚     â€¢ Numeric: Median imputation           â”‚
        â”‚     â€¢ Categorical: Mode imputation         â”‚
        â”‚     â€¢ Drop if >50% missing                 â”‚
        â”‚                                            â”‚
        â”‚  2. Remove duplicates                      â”‚
        â”‚     â€¢ Based on customer_id                 â”‚
        â”‚                                            â”‚
        â”‚  3. Handle outliers                        â”‚
        â”‚     â€¢ IQR method                           â”‚
        â”‚     â€¢ Capping/Flooring                     â”‚
        â”‚                                            â”‚
        â”‚  4. Data type conversion                   â”‚
        â”‚     â€¢ Dates â†’ datetime                     â”‚
        â”‚     â€¢ Categories â†’ categorical             â”‚
        â”‚     â€¢ Numerics â†’ float/int                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   FEATURE ENGINEER (feature_engineer.py)   â”‚
        â”‚                                            â”‚
        â”‚  1. Tenure Features (5 features)           â”‚
        â”‚     â€¢ is_new_customer                      â”‚
        â”‚     â€¢ loyalty_level                        â”‚
        â”‚     â€¢ tenure_months_binned                 â”‚
        â”‚                                            â”‚
        â”‚  2. Financial Features (8 features)        â”‚
        â”‚     â€¢ customer_lifetime_value              â”‚
        â”‚     â€¢ monthly_to_total_ratio               â”‚
        â”‚     â€¢ price_per_service                    â”‚
        â”‚     â€¢ payment_method_risk                  â”‚
        â”‚                                            â”‚
        â”‚  3. Service Features (7 features)          â”‚
        â”‚     â€¢ total_services_count                 â”‚
        â”‚     â€¢ internet_usage_level                 â”‚
        â”‚     â€¢ has_premium_services                 â”‚
        â”‚                                            â”‚
        â”‚  4. Behavioral Features (5 features)       â”‚
        â”‚     â€¢ support_calls_per_month              â”‚
        â”‚     â€¢ contract_type_risk                   â”‚
        â”‚     â€¢ payment_history_score                â”‚
        â”‚                                            â”‚
        â”‚  Output: 25 engineered features            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         READY FOR TRAINING/INFERENCE       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ML Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    START TRAINING                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Load Cleaned & Engineered Data    â”‚
        â”‚   Shape: (100,000 rows, 25 cols)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Train/Test Split (80/20)       â”‚
        â”‚                                     â”‚
        â”‚  Train: 80,000 samples              â”‚
        â”‚  Test:  20,000 samples              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Handle Class Imbalance (SMOTE)   â”‚
        â”‚                                     â”‚
        â”‚  Before: 90% non-churn, 10% churn   â”‚
        â”‚  After:  50% non-churn, 50% churn   â”‚
        â”‚                                     â”‚
        â”‚  Technique: Synthetic Minority      â”‚
        â”‚  Over-sampling (SMOTE)              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Hyperparameter Tuning (GridCV)    â”‚
        â”‚                                     â”‚
        â”‚  Search space:                      â”‚
        â”‚  â€¢ max_depth: [3, 5, 7, 10]         â”‚
        â”‚  â€¢ learning_rate: [0.01, 0.1, 0.3]  â”‚
        â”‚  â€¢ n_estimators: [100, 200, 300]    â”‚
        â”‚  â€¢ subsample: [0.8, 0.9, 1.0]       â”‚
        â”‚                                     â”‚
        â”‚  Method: 5-Fold Cross-Validation    â”‚
        â”‚  Metric: F1-Score                   â”‚
        â”‚                                     â”‚
        â”‚  Total combinations: 4Ã—3Ã—3Ã—3 = 108  â”‚
        â”‚  Time: ~2 hours on 4 cores          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Train Final Model (XGBoost)    â”‚
        â”‚                                     â”‚
        â”‚  Best params:                       â”‚
        â”‚  â€¢ max_depth: 5                     â”‚
        â”‚  â€¢ learning_rate: 0.1               â”‚
        â”‚  â€¢ n_estimators: 200                â”‚
        â”‚  â€¢ subsample: 0.9                   â”‚
        â”‚                                     â”‚
        â”‚  Training time: ~10 minutes         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        Model Evaluation             â”‚
        â”‚                                     â”‚
        â”‚  Test Set Performance:              â”‚
        â”‚  â€¢ Accuracy:  89%                   â”‚
        â”‚  â€¢ Precision: 87%                   â”‚
        â”‚  â€¢ Recall:    91%                   â”‚
        â”‚  â€¢ F1-Score:  89%                   â”‚
        â”‚  â€¢ ROC-AUC:   0.94                  â”‚
        â”‚                                     â”‚
        â”‚  Confusion Matrix:                  â”‚
        â”‚         Pred 0   Pred 1             â”‚
        â”‚  Act 0   16,200    800              â”‚
        â”‚  Act 1      360  2,640              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Feature Importance Analysis    â”‚
        â”‚                                     â”‚
        â”‚  Top 5 Features:                    â”‚
        â”‚  1. tenure_months        (0.18)     â”‚
        â”‚  2. monthly_charges      (0.15)     â”‚
        â”‚  3. support_calls        (0.12)     â”‚
        â”‚  4. contract_type        (0.11)     â”‚
        â”‚  5. internet_service     (0.09)     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Log to MLflow                  â”‚
        â”‚                                     â”‚
        â”‚  â€¢ Parameters                       â”‚
        â”‚  â€¢ Metrics                          â”‚
        â”‚  â€¢ Model artifact                   â”‚
        â”‚  â€¢ Feature importance plot          â”‚
        â”‚  â€¢ Confusion matrix                 â”‚
        â”‚  â€¢ Training dataset metadata        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Save Model                     â”‚
        â”‚                                     â”‚
        â”‚  Formats:                           â”‚
        â”‚  â€¢ model.pkl (joblib)               â”‚
        â”‚  â€¢ model.json (XGBoost native)      â”‚
        â”‚  â€¢ metadata.yaml                    â”‚
        â”‚                                     â”‚
        â”‚  Location: models/ directory        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      TRAINING COMPLETE              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Prediction Flow (Runtime)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLIENT REQUEST                                         â”‚
â”‚  POST /predict                                          â”‚
â”‚  {                                                      â”‚
â”‚    "customer_id": "CUST12345",                          â”‚
â”‚    "age": 42,                                           â”‚
â”‚    "tenure_months": 24,                                 â”‚
â”‚    "monthly_charges": 89.99,                            â”‚
â”‚    ...                                                  â”‚
â”‚  }                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  FastAPI Endpoint              â”‚
        â”‚  @app.post("/predict")         â”‚
        â”‚                                â”‚
        â”‚  â€¢ Receives request            â”‚
        â”‚  â€¢ Timestamp: T0               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Pydantic Validation           â”‚
        â”‚                                â”‚
        â”‚  â€¢ Check data types            â”‚
        â”‚  â€¢ Validate ranges             â”‚
        â”‚  â€¢ Required fields present     â”‚
        â”‚  â€¢ Reject if invalid           â”‚
        â”‚                                â”‚
        â”‚  Time: ~1ms                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Check Redis Cache             â”‚
        â”‚                                â”‚
        â”‚  Key: customer_id              â”‚
        â”‚  TTL: 5 minutes                â”‚
        â”‚                                â”‚
        â”‚  If HIT â†’ return cached result â”‚
        â”‚  If MISS â†’ continue            â”‚
        â”‚                                â”‚
        â”‚  Time: ~2ms                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Cache MISS
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Feature Engineering           â”‚
        â”‚                                â”‚
        â”‚  â€¢ Create derived features     â”‚
        â”‚  â€¢ Same transforms as training â”‚
        â”‚  â€¢ 25 features total           â”‚
        â”‚                                â”‚
        â”‚  Time: ~5ms                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Feature Validation            â”‚
        â”‚                                â”‚
        â”‚  â€¢ Check for NaN/Inf           â”‚
        â”‚  â€¢ Feature ranges              â”‚
        â”‚  â€¢ Data types                  â”‚
        â”‚                                â”‚
        â”‚  Time: ~2ms                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Model Inference (XGBoost)     â”‚
        â”‚                                â”‚
        â”‚  â€¢ Load model from memory      â”‚
        â”‚  â€¢ predict_proba()             â”‚
        â”‚  â€¢ Get churn probability       â”‚
        â”‚                                â”‚
        â”‚  Time: ~30ms                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Post-Processing               â”‚
        â”‚                                â”‚
        â”‚  â€¢ Threshold: 0.5              â”‚
        â”‚  â€¢ Risk level calculation      â”‚
        â”‚  â€¢ Recommendation generation   â”‚
        â”‚                                â”‚
        â”‚  Time: ~1ms                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Log to Database (Async)       â”‚
        â”‚                                â”‚
        â”‚  â€¢ Prediction result           â”‚
        â”‚  â€¢ Features used               â”‚
        â”‚  â€¢ Timestamp                   â”‚
        â”‚  â€¢ Latency                     â”‚
        â”‚                                â”‚
        â”‚  Non-blocking: ~0ms            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Update Monitoring Metrics     â”‚
        â”‚                                â”‚
        â”‚  â€¢ Increment prediction count  â”‚
        â”‚  â€¢ Record latency              â”‚
        â”‚  â€¢ Update distribution         â”‚
        â”‚                                â”‚
        â”‚  Time: ~1ms                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Cache Result in Redis         â”‚
        â”‚                                â”‚
        â”‚  â€¢ Key: customer_id            â”‚
        â”‚  â€¢ Value: prediction result    â”‚
        â”‚  â€¢ TTL: 5 minutes              â”‚
        â”‚                                â”‚
        â”‚  Time: ~2ms                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Return Response               â”‚
        â”‚  {                             â”‚
        â”‚    "customer_id": "CUST12345", â”‚
        â”‚    "churn_probability": 0.23,  â”‚
        â”‚    "will_churn": false,        â”‚
        â”‚    "risk_level": "low",        â”‚
        â”‚    "recommendation": "...",    â”‚
        â”‚    "latency_ms": 45            â”‚
        â”‚  }                             â”‚
        â”‚                                â”‚
        â”‚  Total Time: ~45ms (p95)       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Component Details

### 1. Data Pipeline Components

#### DataLoader (`src/data_pipeline/data_loader.py`)
**Purpose**: Unified interface for loading data from multiple sources

**Class Diagram**:
```
DataLoader
â”œâ”€â”€ __init__(config: dict)
â”œâ”€â”€ load_from_csv(filepath: str) â†’ pd.DataFrame
â”œâ”€â”€ load_from_database(query: str, connection_string: str) â†’ pd.DataFrame
â”œâ”€â”€ load_from_s3(bucket: str, key: str) â†’ pd.DataFrame
â”œâ”€â”€ validate_schema(df: pd.DataFrame) â†’ bool
â””â”€â”€ _handle_errors(func: Callable) â†’ Callable [decorator]

Configuration:
{
  "csv_path": "data/customers.csv",
  "db_type": "postgresql",
  "db_host": "localhost",
  "db_port": 5432,
  "required_columns": ["customer_id", "tenure", "charges", ...]
}
```

**Key Features**:
- Connection pooling for database sources
- Automatic schema validation
- Retry logic for S3 downloads
- Comprehensive logging
- Error handling and recovery

#### DataCleaner (`src/data_pipeline/data_cleaner.py`)
**Purpose**: Data quality and preprocessing

**Cleaning Pipeline**:
```
DataCleaner
â”œâ”€â”€ handle_missing_values(df: pd.DataFrame, strategy: str) â†’ pd.DataFrame
â”‚   â”œâ”€â”€ Numeric: median/mean imputation
â”‚   â”œâ”€â”€ Categorical: mode imputation
â”‚   â””â”€â”€ Drop if >50% missing
â”‚
â”œâ”€â”€ remove_duplicates(df: pd.DataFrame, subset: List[str]) â†’ pd.DataFrame
â”‚   â””â”€â”€ Keep first occurrence
â”‚
â”œâ”€â”€ handle_outliers(df: pd.DataFrame, method: str) â†’ pd.DataFrame
â”‚   â”œâ”€â”€ IQR method (1.5 Ã— IQR)
â”‚   â”œâ”€â”€ Z-score method (threshold: 3)
â”‚   â””â”€â”€ Percentile capping (1st, 99th)
â”‚
â”œâ”€â”€ convert_data_types(df: pd.DataFrame) â†’ pd.DataFrame
â”‚   â”œâ”€â”€ Dates â†’ datetime64
â”‚   â”œâ”€â”€ Categories â†’ category dtype
â”‚   â””â”€â”€ Numerics â†’ float64/int64
â”‚
â””â”€â”€ validate_quality(df: pd.DataFrame) â†’ Dict[str, Any]
    â”œâ”€â”€ Missing value %
    â”œâ”€â”€ Duplicate count
    â”œâ”€â”€ Outlier count
    â””â”€â”€ Data type correctness
```

**Quality Checks**:
- No more than 5% missing values per column
- Zero duplicates on customer_id
- Outliers capped at 1st/99th percentile
- All required columns present

#### FeatureEngineer (`src/data_pipeline/feature_engineer.py`)
**Purpose**: Create predictive features from raw data

**Feature Creation**:
```
FeatureEngineer
â”‚
â”œâ”€â”€ Tenure Features (5 features)
â”‚   â”œâ”€â”€ is_new_customer (tenure < 3 months)
â”‚   â”œâ”€â”€ loyalty_level (New/Regular/Loyal/Champion)
â”‚   â”œâ”€â”€ tenure_months_binned
â”‚   â”œâ”€â”€ tenure_years
â”‚   â””â”€â”€ days_since_signup
â”‚
â”œâ”€â”€ Financial Features (8 features)
â”‚   â”œâ”€â”€ customer_lifetime_value (monthly Ã— tenure)
â”‚   â”œâ”€â”€ monthly_to_total_ratio
â”‚   â”œâ”€â”€ price_per_service
â”‚   â”œâ”€â”€ avg_monthly_spend
â”‚   â”œâ”€â”€ payment_method_risk_score
â”‚   â”œâ”€â”€ has_autopay
â”‚   â”œâ”€â”€ late_payment_count
â”‚   â””â”€â”€ billing_issues_count
â”‚
â”œâ”€â”€ Service Features (7 features)
â”‚   â”œâ”€â”€ total_services_count
â”‚   â”œâ”€â”€ internet_service_type_encoded
â”‚   â”œâ”€â”€ has_premium_services
â”‚   â”œâ”€â”€ service_adoption_rate
â”‚   â”œâ”€â”€ data_usage_level
â”‚   â”œâ”€â”€ phone_usage_level
â”‚   â””â”€â”€ streaming_services_count
â”‚
â””â”€â”€ Behavioral Features (5 features)
    â”œâ”€â”€ support_calls_per_month
    â”œâ”€â”€ contract_type_risk (month-to-month=high)
    â”œâ”€â”€ contract_length_months
    â”œâ”€â”€ has_device_protection
    â””â”€â”€ engagement_score

Total: 25 engineered features
```

### 2. Model Components

#### ChurnModelTrainer (`src/models/train.py`)
**Purpose**: Train, evaluate, and save ML models

**Architecture**:
```
ChurnModelTrainer
â”‚
â”œâ”€â”€ __init__(config: dict)
â”‚   â”œâ”€â”€ Load config (model params, paths)
â”‚   â””â”€â”€ Initialize MLflow tracking
â”‚
â”œâ”€â”€ prepare_data(df: pd.DataFrame) â†’ Tuple
â”‚   â”œâ”€â”€ Feature selection (25 features)
â”‚   â”œâ”€â”€ Train/test split (80/20)
â”‚   â”œâ”€â”€ Feature scaling (StandardScaler)
â”‚   â””â”€â”€ Target variable encoding
â”‚
â”œâ”€â”€ handle_imbalance(X: np.ndarray, y: np.ndarray) â†’ Tuple
â”‚   â”œâ”€â”€ SMOTE (k=5 neighbors)
â”‚   â””â”€â”€ Balance to 50/50 ratio
â”‚
â”œâ”€â”€ hyperparameter_tuning(X, y) â†’ XGBClassifier
â”‚   â”œâ”€â”€ Define parameter grid
â”‚   â”œâ”€â”€ GridSearchCV (5-fold CV)
â”‚   â”œâ”€â”€ Scoring: F1-score
â”‚   â””â”€â”€ Return best estimator
â”‚
â”œâ”€â”€ train_model(X, y) â†’ XGBClassifier
â”‚   â”œâ”€â”€ Fit XGBoost with best params
â”‚   â”œâ”€â”€ Early stopping (10 rounds)
â”‚   â””â”€â”€ Track training time
â”‚
â”œâ”€â”€ evaluate_model(model, X_test, y_test) â†’ Dict
â”‚   â”œâ”€â”€ Accuracy, Precision, Recall, F1
â”‚   â”œâ”€â”€ ROC-AUC
â”‚   â”œâ”€â”€ Confusion Matrix
â”‚   â”œâ”€â”€ Feature Importance
â”‚   â””â”€â”€ Classification Report
â”‚
â”œâ”€â”€ log_to_mlflow(model, metrics, params) â†’ None
â”‚   â”œâ”€â”€ Log hyperparameters
â”‚   â”œâ”€â”€ Log metrics
â”‚   â”œâ”€â”€ Save model artifact
â”‚   â””â”€â”€ Log plots (confusion matrix, feature importance)
â”‚
â””â”€â”€ save_model(model: XGBClassifier, path: str) â†’ None
    â”œâ”€â”€ Joblib format (.pkl)
    â”œâ”€â”€ XGBoost native format (.json)
    â””â”€â”€ Metadata (.yaml)
```

**Hyperparameter Search Space**:
```python
param_grid = {
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.3],
    'n_estimators': [100, 200, 300, 500],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
    'min_child_weight': [1, 3, 5],
    'gamma': [0, 0.1, 0.2]
}
# Total combinations: 4 Ã— 4 Ã— 4 Ã— 4 Ã— 4 Ã— 3 Ã— 3 = 12,288
# With 5-fold CV: 61,440 model fits
# Strategy: Use RandomizedSearchCV with 100 iterations
```

### 3. API Components

#### FastAPI Application (`src/api/app.py`)
**Purpose**: REST API for model serving

**Endpoints**:
```
FastAPI App
â”‚
â”œâ”€â”€ GET /
â”‚   â””â”€â”€ Welcome message and API info
â”‚
â”œâ”€â”€ GET /health
â”‚   â”œâ”€â”€ Status: healthy/unhealthy
â”‚   â”œâ”€â”€ Model loaded: true/false
â”‚   â”œâ”€â”€ Uptime: seconds
â”‚   â””â”€â”€ Version: 1.0.0
â”‚
â”œâ”€â”€ POST /predict
â”‚   â”œâ”€â”€ Input: CustomerData (Pydantic model)
â”‚   â”œâ”€â”€ Validation: automatic via Pydantic
â”‚   â”œâ”€â”€ Process: feature engineering â†’ prediction
â”‚   â””â”€â”€ Output: PredictionResponse
â”‚       â”œâ”€â”€ customer_id
â”‚       â”œâ”€â”€ churn_probability
â”‚       â”œâ”€â”€ will_churn
â”‚       â”œâ”€â”€ risk_level
â”‚       â”œâ”€â”€ recommendation
â”‚       â””â”€â”€ timestamp
â”‚
â”œâ”€â”€ POST /predict/batch
â”‚   â”œâ”€â”€ Input: List[CustomerData]
â”‚   â”œâ”€â”€ Max batch size: 1000
â”‚   â”œâ”€â”€ Process: parallel predictions
â”‚   â””â”€â”€ Output: BatchPredictionResponse
â”‚       â”œâ”€â”€ predictions: List[PredictionResponse]
â”‚       â”œâ”€â”€ total_processed
â”‚       â”œâ”€â”€ high_risk_count
â”‚       â””â”€â”€ processing_time_ms
â”‚
â”œâ”€â”€ GET /metrics
â”‚   â”œâ”€â”€ Prometheus format
â”‚   â”œâ”€â”€ Total predictions
â”‚   â”œâ”€â”€ Average latency
â”‚   â”œâ”€â”€ Error rate
â”‚   â””â”€â”€ Prediction distribution
â”‚
â””â”€â”€ GET /docs
    â””â”€â”€ Auto-generated Swagger UI
```

**Request/Response Models**:
```python
class CustomerData(BaseModel):
    customer_id: str
    age: int = Field(ge=18, le=100)
    tenure_months: int = Field(ge=0)
    monthly_charges: float = Field(gt=0)
    total_charges: float = Field(ge=0)
    support_calls: int = Field(ge=0)
    internet_service: str = Field(regex="^(DSL|Fiber|None)$")
    contract: str = Field(regex="^(Month-to-month|One year|Two year)$")
    payment_method: str

class PredictionResponse(BaseModel):
    customer_id: str
    churn_probability: float = Field(ge=0, le=1)
    will_churn: bool
    risk_level: str = Field(regex="^(low|medium|high)$")
    recommendation: str
    timestamp: datetime
    latency_ms: float
```

### 4. Monitoring Components

#### MetricsTracker (`src/monitoring/metrics.py`)
**Purpose**: Track model performance and detect issues

**Monitoring Modules**:
```
MetricsTracker
â”‚
â”œâ”€â”€ PredictionMetrics
â”‚   â”œâ”€â”€ total_predictions (Counter)
â”‚   â”œâ”€â”€ prediction_latency (Histogram)
â”‚   â”œâ”€â”€ churn_probability_distribution (Histogram)
â”‚   â”œâ”€â”€ error_count (Counter)
â”‚   â””â”€â”€ cache_hit_rate (Gauge)
â”‚
â”œâ”€â”€ ModelPerformanceMetrics
â”‚   â”œâ”€â”€ accuracy (Gauge)
â”‚   â”œâ”€â”€ precision (Gauge)
â”‚   â”œâ”€â”€ recall (Gauge)
â”‚   â”œâ”€â”€ f1_score (Gauge)
â”‚   â””â”€â”€ roc_auc (Gauge)
â”‚
â”œâ”€â”€ DataDriftDetector
â”‚   â”œâ”€â”€ calculate_psi(expected, actual) â†’ float
â”‚   â”‚   â””â”€â”€ Population Stability Index
â”‚   â”œâ”€â”€ detect_drift(features) â†’ bool
â”‚   â”‚   â””â”€â”€ PSI > 0.25 = drift
â”‚   â””â”€â”€ alert_on_drift() â†’ None
â”‚
â”œâ”€â”€ PerformanceDegradationDetector
â”‚   â”œâ”€â”€ compare_to_baseline(current, baseline) â†’ Dict
â”‚   â”œâ”€â”€ check_thresholds(metrics) â†’ List[Alert]
â”‚   â””â”€â”€ trigger_retraining() â†’ None
â”‚
â””â”€â”€ AlertManager
    â”œâ”€â”€ send_slack_alert(message)
    â”œâ”€â”€ send_pagerduty_alert(incident)
    â””â”€â”€ send_email_alert(recipients, message)
```

**Drift Detection Example**:
```python
# PSI Calculation
def calculate_psi(expected, actual, bins=10):
    # Divide into bins
    expected_freq = np.histogram(expected, bins=bins)[0] / len(expected)
    actual_freq = np.histogram(actual, bins=bins)[0] / len(actual)
    
    # PSI formula
    psi = np.sum(
        (actual_freq - expected_freq) * 
        np.log((actual_freq + 0.0001) / (expected_freq + 0.0001))
    )
    
    return psi

# Interpretation
if psi < 0.1:
    status = "No significant change"
elif psi < 0.25:
    status = "Some change detected - monitor closely"
else:
    status = "Significant drift - retrain model!"
```

---

## ğŸŒŠ Data Flow

### End-to-End Data Journey

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DAY 0: INITIAL TRAINING                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Historical Data (6 months)
    â†“
Load 100,000 customer records
    â†“
Clean data (handle missing, outliers, duplicates)
    â†“
Engineer 25 features
    â†“
Split 80/20 (train/test)
    â†“
SMOTE balancing on training set
    â†“
Hyperparameter tuning (GridSearchCV, 5-fold CV)
    â†“
Train XGBoost model
    â†“
Evaluate on test set: 89% accuracy
    â†“
Save model.pkl + metadata
    â†“
Log to MLflow
    â†“
MODEL READY FOR DEPLOYMENT

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DAY 1-30: PRODUCTION SERVING                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Customer website sends API request
    â†“
FastAPI receives POST /predict
    â†“
Pydantic validates input
    â†“
Check Redis cache (hit rate: ~30%)
    â†“ [Cache miss]
Feature engineering (5ms)
    â†“
Model inference (30ms)
    â†“
Post-processing (risk level, recommendation)
    â†“
Log to PostgreSQL (async)
    â†“
Update Prometheus metrics
    â†“
Cache result in Redis (5min TTL)
    â†“
Return response to client
    â†“
TOTAL: ~45ms latency

Daily: 50,000 predictions
Weekly ground truth: Check actual churn vs predicted

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DAY 30: MONITORING DETECTS DRIFT                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Monitoring job runs daily
    â†“
Compare last 7 days data to training data
    â†“
Calculate PSI for all 25 features
    â†“
Feature "tenure_months" has PSI = 0.28 (>0.25 threshold!)
    â†“
DRIFT DETECTED
    â†“
Alert sent to Slack: "Drift detected in tenure_months"
    â†“
Check model performance: accuracy dropped from 89% â†’ 85%
    â†“
TRIGGER RETRAINING

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DAY 31: AUTOMATED RETRAINING                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Extract last 6 months of production data
    â†“
Include predictions + ground truth labels
    â†“
Run full data pipeline (clean, engineer features)
    â†“
Train new model with same pipeline
    â†“
Evaluate new model: 88% accuracy
    â†“
Compare to current model (85% accuracy)
    â†“
NEW MODEL IS BETTER (+3%)
    â†“
A/B test: 10% traffic to new model for 24 hours
    â†“
Monitor performance in production
    â†“
New model performs well in A/B test
    â†“
Deploy new model to 100% traffic
    â†“
Save old model as fallback (model_v1.pkl)
    â†“
Update MLflow with new model version
    â†“
RETRAINING COMPLETE

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CONTINUOUS CYCLE                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Serve predictions â†’ Collect ground truth â†’ Monitor drift â†’
Retrain when needed â†’ Deploy new model â†’ Repeat
```

---

## ğŸ› ï¸ Technology Stack

### Core ML Stack
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| ML Framework | XGBoost | 2.0+ | Gradient boosting classifier |
| Data Processing | Pandas | 2.0+ | Data manipulation |
| Numerical Computing | NumPy | 1.24+ | Array operations |
| ML Pipeline | Scikit-learn | 1.3+ | Preprocessing, metrics, SMOTE |
| Visualization | Matplotlib, Seaborn | Latest | Plots and charts |

### API & Web Stack
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| Web Framework | FastAPI | 0.104+ | REST API |
| ASGI Server | Uvicorn | 0.24+ | Production server |
| Validation | Pydantic | 2.0+ | Data validation |
| HTTP Client | Requests | 2.31+ | External API calls |

### Data Storage
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| Database | PostgreSQL | 14+ | Predictions, ground truth |
| Cache | Redis | 7.0+ | Prediction caching |
| Object Storage | AWS S3 / MinIO | Latest | Model artifacts, data |

### Monitoring & Observability
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| Metrics | Prometheus | 2.45+ | Time-series metrics |
| Dashboards | Grafana | 10.0+ | Visualization |
| Experiment Tracking | MLflow | 2.8+ | Model versioning |
| Logging | Python logging | Built-in | Application logs |

### DevOps & Infrastructure
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| Containerization | Docker | 24.0+ | Packaging |
| Orchestration | Docker Compose | 2.20+ | Multi-container apps |
| CI/CD | GitHub Actions | Latest | Automation |
| Testing | Pytest | 7.4+ | Unit/integration tests |
| Code Quality | Black, Flake8, MyPy | Latest | Linting, formatting |

### Dependencies Overview
```
# Production Dependencies (requirements.txt)
xgboost==2.0.2           # ML model
fastapi==0.104.1         # Web framework
uvicorn==0.24.0          # ASGI server
pydantic==2.5.0          # Validation
pandas==2.1.3            # Data processing
numpy==1.24.3            # Numerical computing
scikit-learn==1.3.2      # ML utilities
imbalanced-learn==0.11.0 # SMOTE
prometheus-client==0.19.0 # Metrics
mlflow==2.8.1            # Experiment tracking
psycopg2-binary==2.9.9   # PostgreSQL
redis==5.0.1             # Redis client
boto3==1.29.7            # AWS SDK
pyyaml==6.0.1            # Config files

# Development Dependencies
pytest==7.4.3
pytest-cov==4.1.0
black==23.11.0
flake8==6.1.0
mypy==1.7.1
```

---

## ğŸš€ Deployment Architecture

### Local Development
```
Developer Machine
â”œâ”€â”€ Python Virtual Environment
â”œâ”€â”€ Local PostgreSQL (optional)
â”œâ”€â”€ Local Redis (optional)
â””â”€â”€ Docker Compose (recommended)
    â”œâ”€â”€ API container
    â”œâ”€â”€ PostgreSQL container
    â”œâ”€â”€ Redis container
    â”œâ”€â”€ MLflow container
    â”œâ”€â”€ Prometheus container
    â””â”€â”€ Grafana container
```

### Staging Environment
```
AWS/GCP/Azure
â”œâ”€â”€ ECS/Cloud Run/Container Instances
â”‚   â”œâ”€â”€ API containers (2 replicas)
â”‚   â”œâ”€â”€ PostgreSQL RDS/Cloud SQL
â”‚   â”œâ”€â”€ Redis ElastiCache/MemoryStore
â”‚   â””â”€â”€ Load Balancer
â”œâ”€â”€ S3/GCS/Blob Storage
â”‚   â”œâ”€â”€ Model artifacts
â”‚   â””â”€â”€ Training data
â””â”€â”€ CloudWatch/Stackdriver/Monitor
    â”œâ”€â”€ Logs
    â””â”€â”€ Metrics
```

### Production Environment (AWS Example)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Route 53 (DNS)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Application Load Balancer (ALB)               â”‚
â”‚  â€¢ SSL/TLS termination                              â”‚
â”‚  â€¢ Health checks                                    â”‚
â”‚  â€¢ Path-based routing                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚            â”‚            â”‚
          â–¼            â–¼            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ECS     â”‚  â”‚ ECS     â”‚  â”‚ ECS     â”‚
    â”‚ Task 1  â”‚  â”‚ Task 2  â”‚  â”‚ Task 3  â”‚
    â”‚ (API)   â”‚  â”‚ (API)   â”‚  â”‚ (API)   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚            â”‚            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚           â”‚           â”‚
          â–¼           â–¼           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ RDS      â”‚  â”‚Redis â”‚  â”‚ S3       â”‚
    â”‚Postgres  â”‚  â”‚Elastiâ”‚  â”‚ (Models) â”‚
    â”‚          â”‚  â”‚Cache â”‚  â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      CloudWatch                     â”‚
    â”‚  â€¢ Logs                             â”‚
    â”‚  â€¢ Metrics                          â”‚
    â”‚  â€¢ Alarms                           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Kubernetes Deployment (Alternative)
```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: churn-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: churn-api
  template:
    spec:
      containers:
      - name: api
        image: churn-api:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "2000m"
            memory: "4Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: url
---
apiVersion: v1
kind: Service
metadata:
  name: churn-api-service
spec:
  type: LoadBalancer
  selector:
    app: churn-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
```

---

## âš¡ Scalability & Performance

### Current Performance
- **Latency**: 45ms p95, 80ms p99
- **Throughput**: 50,000 predictions/day (~0.6 req/sec avg)
- **Availability**: 99.9% uptime
- **Cache Hit Rate**: ~30%

### Scaling Strategies

#### Horizontal Scaling (More Containers)
```
Current: 1 container, 0.6 req/sec
         â†“
Scale to: 3 containers, 1.8 req/sec (3x)
         â†“
Scale to: 10 containers, 6 req/sec (10x)
         â†“
Scale to: 50 containers, 30 req/sec (50x)

Maximum with current architecture: ~100 containers
= 60 req/sec = 5.2M predictions/day
```

#### Vertical Scaling (Bigger Containers)
```
Current: 1 CPU, 2GB RAM â†’ 0.6 req/sec
         â†“
Scale to: 2 CPU, 4GB RAM â†’ 1.2 req/sec (2x)
         â†“
Scale to: 4 CPU, 8GB RAM â†’ 2.4 req/sec (4x)

Diminishing returns after 4 CPUs for this workload
```

#### Caching Optimization
```
Current: 30% cache hit rate, Redis TTL=5min
         â†“
Increase TTL: 30min â†’ 60% hit rate
         â†“
Smart caching: Cache high-volume customers â†’ 80% hit rate
         â†“
Result: 3x latency improvement for cached requests
```

#### Batch Processing
```
Current: 1 prediction per request = 45ms
         â†“
Batch endpoint: 100 predictions per request = 2000ms
         â†“
Per-prediction latency: 2000ms / 100 = 20ms (2.25x faster!)
         â†“
Throughput: 100x higher for batch use cases
```

### Capacity Planning

**Small Scale (Startup)**:
- Traffic: 10K predictions/day
- Infrastructure: 1 API container, 1 DB, 1 Redis
- Cost: ~$100/month (AWS)

**Medium Scale (Growing Company)**:
- Traffic: 1M predictions/day
- Infrastructure: 10 API containers, RDS Multi-AZ, ElastiCache cluster
- Cost: ~$2,000/month (AWS)

**Large Scale (Enterprise)**:
- Traffic: 100M predictions/day
- Infrastructure: Auto-scaling (10-100 containers), Aurora Serverless, ElastiCache cluster
- Cost: ~$20,000/month (AWS)

---

## ğŸ”’ Security

### Authentication & Authorization
```python
# API Key Authentication
from fastapi.security import APIKeyHeader

api_key_header = APIKeyHeader(name="X-API-Key")

@app.post("/predict")
async def predict(api_key: str = Depends(api_key_header)):
    if api_key not in valid_api_keys:
        raise HTTPException(status_code=403, detail="Invalid API key")
    # ... rest of endpoint
```

### Data Privacy
- **PII Handling**: Customer IDs are hashed before logging
- **Encryption**: TLS 1.3 for data in transit
- **Data Retention**: Predictions stored for 90 days, then archived
- **GDPR Compliance**: Right to deletion implemented

### Model Security
- **Model Versioning**: All models tracked in MLflow with checksums
- **Rollback**: Automatic rollback if new model has >10% error rate
- **Input Validation**: Pydantic validates all inputs to prevent injection attacks
- **Rate Limiting**: 1000 req/min per API key

### Infrastructure Security
```yaml
# docker-compose.yml security
services:
  api:
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp
    environment:
      - PYTHONDONTWRITEBYTECODE=1
```

---

## ğŸ“Š Monitoring & Observability

### Prometheus Metrics
```python
# Custom metrics
from prometheus_client import Counter, Histogram, Gauge

# Prediction metrics
prediction_counter = Counter(
    'churn_predictions_total', 
    'Total predictions made',
    ['risk_level']
)

prediction_latency = Histogram(
    'churn_prediction_latency_seconds',
    'Prediction latency',
    buckets=[0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]
)

model_accuracy = Gauge(
    'churn_model_accuracy',
    'Current model accuracy'
)
```

### Grafana Dashboards

**Dashboard 1: API Performance**
- Request rate (req/sec)
- Latency percentiles (p50, p95, p99)
- Error rate
- Cache hit rate

**Dashboard 2: ML Model Performance**
- Prediction distribution (churn vs non-churn)
- Model accuracy over time
- Feature drift scores (PSI)
- Retraining events

**Dashboard 3: Infrastructure**
- CPU usage
- Memory usage
- Database connections
- Container health

### Alerts
```yaml
# alerts.yml
groups:
- name: churn_api
  rules:
  - alert: HighLatency
    expr: histogram_quantile(0.95, prediction_latency) > 0.1
    for: 5m
    annotations:
      summary: "API latency is high"
  
  - alert: ModelDrift
    expr: feature_psi > 0.25
    annotations:
      summary: "Data drift detected"
  
  - alert: LowAccuracy
    expr: model_accuracy < 0.85
    for: 1h
    annotations:
      summary: "Model accuracy degraded"
```

---

## ğŸ”„ CI/CD Pipeline

### GitHub Actions Workflow
```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Run tests
        run: pytest tests/ --cov=src --cov-report=xml
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3

  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: pip install black flake8 mypy
      - run: black --check src/
      - run: flake8 src/
      - run: mypy src/

  build:
    needs: [test, lint]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Build Docker image
        run: docker build -t churn-api:${{ github.sha }} .
      
      - name: Push to registry
        run: |
          echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
          docker push churn-api:${{ github.sha }}

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Deploy to staging
        run: |
          # Update ECS service with new image
          aws ecs update-service --cluster staging --service churn-api --force-new-deployment
```

### Deployment Stages
1. **Commit** â†’ Push to GitHub
2. **CI** â†’ Run tests, linting, type checking
3. **Build** â†’ Create Docker image
4. **Deploy to Staging** â†’ Automatic for main branch
5. **Integration Tests** â†’ Run against staging
6. **Manual Approval** â†’ Required for production
7. **Deploy to Production** â†’ Blue-green deployment
8. **Smoke Tests** â†’ Verify production health
9. **Monitor** â†’ Watch metrics for 1 hour

---

## ğŸ“š Summary

This architecture provides:
- âœ… **Production-ready** ML system
- âœ… **Scalable** to millions of predictions
- âœ… **Monitored** with automated alerts
- âœ… **Maintainable** with clean code structure
- âœ… **Resilient** with error handling and fallbacks
- âœ… **Secure** with authentication and encryption
- âœ… **Automated** CI/CD and retraining

**Total System Complexity**: Production-grade ML engineering
**Estimated Build Time**: 2-3 weeks for 1 engineer
**Maintenance**: ~4 hours/week

---

*For more details, see [BEGINNER_GUIDE.md](BEGINNER_GUIDE.md) and [HOW_TO_USE.md](HOW_TO_USE.md)*
